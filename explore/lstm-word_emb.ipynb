{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> LSTM Decoder with word vector representation </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "input_file = \"../datasets/chapter01.txt\"\n",
    "with codecs.open(input_file, \"r\", encoding=None) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_wordlist = text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\n’', lower=True, split=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(keras_wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2707"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keras_wordlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word set extraction from specific corpus and word vector transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def get_wordset(saves_folder, vocab_name):\n",
    "    vocab_filename = os.path.join(saves_folder, vocab_name + \".pkl\")\n",
    "    with open(vocab_filename, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    return vocab['words_set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params\n",
    "saves_folder = '../saves'\n",
    "vocab = 'vocab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordset = get_wordset(saves_folder, vocab)\n",
    "wordlist = list(wordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import word vectors\n",
    "from gensim.models.fasttext import FastText as ftext\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "wv_model = ftext.load_fasttext_format(\"../embeddings/fasttext/it/it\")\n",
    "fastvec = KeyedVectors.load_word2vec_format(\"../embeddings/fasttext/it/it.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disegnare\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('disegnò', 0.7645198106765747),\n",
       " ('disegnando', 0.7410539388656616),\n",
       " ('disegna', 0.7338820695877075),\n",
       " ('disegnate', 0.6761715412139893),\n",
       " ('disegnato', 0.6728403568267822),\n",
       " ('disegnati', 0.6685545444488525),\n",
       " ('disegnatore', 0.6536780595779419),\n",
       " ('disegnata', 0.6432667970657349),\n",
       " ('dipingere', 0.6279634833335876),\n",
       " ('disegnatori', 0.6200643181800842)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test similar word\n",
    "testword = wordlist[0]\n",
    "print(testword)\n",
    "wv_model.wv.most_similar(testword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve vector representation of test word\n",
    "testword_vec = wv_model.wv.get_vector(testword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testword_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_vecs(dict_wordset, model):\n",
    "    vecs = []\n",
    "    for word in dict_wordset:\n",
    "        vec = model.wv.get_vector(word.lower)\n",
    "        vecs.append(vec)\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_vecs = dict_to_vecs(wordset, wv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_vecs) == len(wordset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize with a little help from spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"it_core_news_sm\")\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_in_doc = len(max(doc.sents, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quella specie di orologio da taschino, pesante e piuttosto ammaccato, serviva a dargli un’aria importante, dicevano.\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for sentence in doc.sents:\n",
    "    sentences.append(sentence)\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with Embedding Layer and pre-trained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 20\n",
      "[[4, 5], [6, 1], [2, 7, 3], [2, 8, 1], [9], [10], [11, 3], [12, 13], [2, 1, 14], [15, 16, 17, 18, 19]]\n",
      "[[ 4  5  0  0]\n",
      " [ 6  1  0  0]\n",
      " [ 2  7  3  0]\n",
      " [ 2  8  1  0]\n",
      " [ 9  0  0  0]\n",
      " [10  0  0  0]\n",
      " [11  3  0  0]\n",
      " [12 13  0  0]\n",
      " [ 2  1 14  0]\n",
      " [16 17 18 19]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "# define documents\n",
    "docs = ['Ben fatto!',\n",
    "    'Ottimo lavoro',\n",
    "    'Un grande sforzo',\n",
    "    'un buon lavoro',\n",
    "    'Eccellente!',\n",
    "    'Debole',\n",
    "    'Poco sforzo!',\n",
    "    'non bene',\n",
    "    'un lavoro povero',\n",
    "    'Si poteva fare di meglio.']\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print(\"vocab_size\", vocab_size)\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('../embeddings/fasttext/it/it.vec')\n",
    "line_num = 0\n",
    "for line in f:\n",
    "    line_num += 1\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    if line_num == 100:\n",
    "        break\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['a'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, wordvec_size))\n",
    "for word, i in t.word_index.items():\n",
    "    #embedding_vector = embeddings_index.get(word)\n",
    "    embedding_vector = wv_model.wv.get_vector(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape == (vocab_size, wordvec_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 300)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, wordvec_size, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 4, 300)            6000      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1201      \n",
      "=================================================================\n",
      "Total params: 7,201\n",
      "Trainable params: 1,201\n",
      "Non-trainable params: 6,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(e)\n",
    "model.add(\n",
    "    LSTM(512, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(vocab_size))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_tensor, y_tensor, batch_size=128, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
