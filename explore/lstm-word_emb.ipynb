{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> LSTM Decoder with word vector representation </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load gensim italian lang wordvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import word vectors\n",
    "from gensim.models.fasttext import FastText as ftext\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "wv_model = ftext.load_fasttext_format(\"../embeddings/fasttext/it/it\")\n",
    "fastvec = KeyedVectors.load_word2vec_format(\"../embeddings/fasttext/it/it.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### attempt - word set extraction from specific corpus and word vector transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def get_wordset(saves_folder, vocab_name):\n",
    "    vocab_filename = os.path.join(saves_folder, vocab_name + \".pkl\")\n",
    "    with open(vocab_filename, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    return vocab['words_set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params\n",
    "saves_folder = '../saves'\n",
    "vocab = 'vocab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordset = get_wordset(saves_folder, vocab)\n",
    "wordlist = list(wordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test similar word\n",
    "testword = wordlist[0]\n",
    "print(testword)\n",
    "wv_model.wv.most_similar(testword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve vector representation of test word\n",
    "testword_vec = wv_model.wv.word_vec(testword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testword_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### transform words in dictionary into word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_vecs(dict_wordset, model):\n",
    "    vecs = []\n",
    "    for word in dict_wordset:\n",
    "        vec = model.wv.word_vec(str(word.lower))\n",
    "        vecs.append(vec)\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_vecs = dict_to_vecs(wordset, wv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_vecs) == len(wordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dict_vecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "input_file = \"../datasets/chapter01.txt\"\n",
    "with codecs.open(input_file, \"r\", encoding=None) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### attempt 1 - keras tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_wordlist = text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\n’', lower=True, split=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(keras_wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(keras_wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "unique_wordset = np.unique(keras_wordlist)\n",
    "unique_wordset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### attempt 2 - derive text properties with a little help from spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"it_core_news_sm\")\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_in_doc = len(max(doc.sents, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for sentence in doc.sents:\n",
    "    sentences.append(sentence)\n",
    "print(sentences[201])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence of max num of words\n",
    "sentence_maxlen = len(max(doc.sents, key=len))\n",
    "print(sentence_maxlen)\n",
    "\n",
    "sentence_minlen = len(min(doc.sents, key=len))\n",
    "print(sentence_minlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = []\n",
    "for token in doc:\n",
    "    if token.like_num:\n",
    "        numbers.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if numbers are found, convert into words thru this map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_to_words = {\n",
    "  '1974':'millenovecentosettantaquattro',\n",
    "  '1976':'millenovecentosettantasei',\n",
    "  '1978':'millenovecentosettantotto',\n",
    "  '1979':'millenovecentosettantanove',\n",
    "  '1980':'millenovecentottanta',\n",
    "  '1984':'millenovecentottantaquattro',\n",
    "  '1992':'millenovecentonovantadue',\n",
    "  '14':'quattordici',\n",
    "  '7':'sette',\n",
    "  '13':'tredici',\n",
    "  '1981':'millenovecentottantuno',\n",
    "  '1982':'millenovecentottandue',\n",
    "  '47':'quarantasette',\n",
    "  '9.30':'nove e trenta',\n",
    "  '7':'sette',\n",
    "  '22':'ventidue',\n",
    "  '7.10':'sette e dieci',\n",
    "  '9':'nove',\n",
    "  '10':'dieci' \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare X, y with \"brute\" sampling : fixed length sentence chunk as X_i, single next word as y_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace unwanted chars\n",
    "processed = text.replace('«', '')\n",
    "processed = processed.replace('»', '')\n",
    "processed = processed.replace(' - ', ' ')\n",
    "\n",
    "\n",
    "for key in number_to_words.keys():\n",
    "    processed = processed.replace(key, number_to_words[key])\n",
    "    \n",
    "processed = processed.replace(\"où\", '')\n",
    "processed = processed.replace(\"\\x1a\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling sentences with len (words): 30 with sampling step window: 3\n",
      "nb sequences(length of sentences): 872\n",
      "length of next_word 872\n"
     ]
    }
   ],
   "source": [
    "max_length = 30\n",
    "\n",
    "def sample_sentences(text, sample_len, sample_step):\n",
    "\n",
    "    print(\"Sampling sentences with len (words):\", sample_len, \"with sampling step window:\", sample_step)\n",
    "    sampled_sentences = []\n",
    "    sampled_next_words = []\n",
    "\n",
    "    list_words = text.split()\n",
    "\n",
    "    for pos in range(0, len(list_words) - sample_len, sample_step):\n",
    "        temp = ' '.join(list_words[pos: pos + sample_len])\n",
    "        sampled_sentences.append(temp)\n",
    "        sampled_next_words.append((list_words[pos + sample_len]))\n",
    "    print('nb sequences(length of sentences):', len(sampled_sentences))\n",
    "    print(\"length of next_word\", len(sampled_next_words))\n",
    "\n",
    "    return sampled_sentences, sampled_next_words\n",
    "\n",
    "X_sentences, y_next_words = sample_sentences(processed, max_length, 3)\n",
    "\n",
    "#         print('Vectorizing...')\n",
    "#         num_sentences = len(sentences)\n",
    "#         words_in_sentence = sampling_maxlen\n",
    "\n",
    "#         X = np.zeros((num_sentences, words_in_sentence, dict_len), dtype=np.bool)\n",
    "#         y = np.zeros((num_sentences, dict_len), dtype=np.bool)\n",
    "#         for i, sentence in enumerate(sentences):\n",
    "#             for t, word in enumerate(sentence.split()):\n",
    "#                 # print(i, t, word)\n",
    "#                 X[i, t, word_indices[word]] = 1\n",
    "#                 y[i, word_indices[next_words[i]]] = 1\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "che ricorda ogni cosa, e ci chiama con l’odore del sale. Lì, finalmente, Seurac si fermava, adagiandosi sullo stesso sasso, poche ore dopo un misero pranzo; che ci fosse pioggia\n",
      "o\n"
     ]
    }
   ],
   "source": [
    "print(X_sentences[100])\n",
    "print(y_next_words[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuel/miniconda3/envs/wordgen-explore/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quella specie di orologio da taschino, pesante e piuttosto ammaccato, serviva a dargli un’aria importante, dicevano. Se lo portava di qua, di là, arrancando, su e giù per le strade\n",
      "[54, 317, 3, 88, 13, 1039, 1037, 1, 314, 1035, 1036, 5, 312, 1033, 313, 311, 15, 44, 120, 3, 1029, 3, 1026, 1027, 86, 1, 176, 12, 9, 179]\n",
      "1040\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tt = Tokenizer()\n",
    "tt.fit_on_texts(X_sentences)\n",
    "X_sentences_encoded = tt.texts_to_sequences(X_sentences)\n",
    "print(X_sentences[0])\n",
    "print(X_sentences_encoded[0])\n",
    "print(len(tt.word_index) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[321, 12, 123, 180, 13, 37, 90, 10, 56, 322, 6, 323, 181, 1, 21, 182, 5, 91, 71, 2, 27, 13, 91, 324, 38, 55, 22, 8, 325, 92]\n",
      "[321  12 123 180  13  37  90  10  56 322   6 323 181   1  21 182   5  91\n",
      "  71   2  27  13  91 324  38  55  22   8 325  92]\n"
     ]
    }
   ],
   "source": [
    "# if needed: in this case all sentences are of equal size of max_length\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X_sentences_encoded_padded = pad_sequences(X_sentences_encoded, maxlen=max_length, padding='post')\n",
    "\n",
    "print(X_sentences_encoded[12])\n",
    "print(X_sentences_encoded_padded[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  54  317    3 ...   12    9  179]\n",
      " [  88   13 1039 ...    3   89   14]\n",
      " [1037    1  314 ...  318  319  320]\n",
      " ...\n",
      " [  29 1022    1 ...    8  315 1038]\n",
      " [1023   85   68 ...    2   17  316]\n",
      " [  24 1024    9 ...  122   55   26]]\n"
     ]
    }
   ],
   "source": [
    "print(X_sentences_encoded_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['di', 'pratica', 'inseparabili.', 'viveva', 'parti']\n",
      "[[3], [318], [321], [180], [90]]\n"
     ]
    }
   ],
   "source": [
    "y_next_words_encoded = tt.texts_to_sequences(y_next_words)\n",
    "y_next_words_encoded_padded = pad_sequences(y_next_words_encoded, maxlen=1, padding='post')\n",
    "\n",
    "print(y_next_words[:5])\n",
    "print(y_next_words_encoded[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3]\n",
      " [318]\n",
      " [321]\n",
      " [180]\n",
      " [ 90]]\n"
     ]
    }
   ],
   "source": [
    "print(y_next_words_encoded_padded[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(872, 30)\n",
      "(872, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_sentences_encoded_padded.shape)\n",
    "print(y_next_words_encoded_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(872,)\n"
     ]
    }
   ],
   "source": [
    "# try to flatten y\n",
    "import numpy as np\n",
    "y_next_words_encoded_flattened = np.array(y_next_words_encoded).flatten()\n",
    "print(y_next_words_encoded_flattened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with a word vec representation of y\n",
    "y_next_words_vectorized = []\n",
    "for next_word in y_next_words:\n",
    "    y_next_words_vectorized.append(wv_model.wv.word_vec(next_word.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(872, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_next_words_vectorized).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### another way for data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'e' not in wv_model.wv.vocab:\n",
    "    simword = wv_model.wv.most_similar('e')[0][0]\n",
    "    print(word2idx(simword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sentences: 114\n",
      "train_x shape: (114, 30)\n",
      "train_y shape: (114,)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "def word2idx(word):\n",
    "    if word not in wv_model.wv.vocab:\n",
    "        simword = wv_model.wv.most_similar(word)[0][0]\n",
    "        return wv_model.wv.vocab[simword].index\n",
    "    return wv_model.wv.vocab[word].index\n",
    "\n",
    "def idx2word(idx):\n",
    "    return wv_model.wv.index2word[idx]\n",
    "\n",
    "\n",
    "max_sentence_len = 30\n",
    "\n",
    "path = \"../datasets/chapter01.txt\"\n",
    "with open(path) as file_:\n",
    "    docs = file_.readlines()\n",
    "sentences = [[word for word in doc.lower().translate(string.punctuation).split()[:max_sentence_len]] for doc in docs]\n",
    "print('Num sentences:', len(sentences))\n",
    "\n",
    "train_x = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
    "train_y = np.zeros([len(sentences)], dtype=np.int32)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, word in enumerate(sentence[:-1]):\n",
    "        word = word.replace('\"', '')\n",
    "        train_x[i, t] = word2idx(word)\n",
    "    train_y[i] = word2idx(sentence[-1])\n",
    "print('train_x shape:', train_x.shape)\n",
    "print('train_y shape:', train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1406, 20725,   288, 12556,    98,    12,  5083, 21884,  2012,\n",
       "          17,    30,   945, 31715,  1571,     5, 44335, 40119,   642,\n",
       "         130,   999,  7251,  3018, 31969,  9358, 25384,  2045, 38801,\n",
       "       36701, 40363,   622, 39005, 11171, 17873, 10817,  4590, 18928,\n",
       "       19629, 36914, 22194, 11880, 47097, 39252, 33699,  5380, 27236,\n",
       "         102,  6081,  6081,   567, 19369, 13280, 25760, 35723,  4759,\n",
       "        6537, 32268,  2194,  3810,  4588,  2133,  9970,  1266, 18210,\n",
       "         944, 13167,  5806,  4562,  4006, 35723, 39702,  3209, 30275,\n",
       "        5788, 18928, 37228, 12950,  4759,  5477,  1319, 40642, 18492,\n",
       "       20134, 14759, 21884,   931, 30770, 44335,  5058, 38589, 12261,\n",
       "       40012, 44335, 15105, 44335,    97, 47469,   249,   377,   863,\n",
       "           7,  6408, 29664,   239, 38406, 27748, 20195, 13008,   188,\n",
       "        1359, 10760, 45096, 39835, 20134, 11759], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 1040\n",
      "wordvec_size 300\n"
     ]
    }
   ],
   "source": [
    "# use same tokenizer\n",
    "#t = Tokenizer()\n",
    "#t.fit_on_texts(X_sentences)\n",
    "vocab_size = len(tt.word_index) + 1\n",
    "wordvec_size = 300\n",
    "print(\"vocab_size\", vocab_size)\n",
    "print(\"wordvec_size\", wordvec_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, wordvec_size))\n",
    "for word, i in tt.word_index.items():\n",
    "    #embedding_vector = embeddings_index.get(word)\n",
    "    embedding_vector = wv_model.wv.word_vec(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape == (vocab_size, wordvec_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1040, 300)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if indexed word in text is properly embedded with the correct word vector from gensim\n",
    "test_word = 'pratica'\n",
    "word_index = tt.word_index[test_word]\n",
    "print(word_index)\n",
    "np.array_equal(wv_model.wv.get_vector(test_word), embedding_matrix[word_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuel/miniconda3/envs/wordgen-explore/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50032, 300)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n",
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "print(tf.__version__) #1.8.0\n",
    "print(keras.__version__) #2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, wordvec_size, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "model.add(e)\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(vocab_size))\n",
    "#model.add(Dense(1))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 30, 300)           312000    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 30, 512)           1665024   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 30, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1040)              533520    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1040)              0         \n",
      "=================================================================\n",
      "Total params: 4,609,744\n",
      "Trainable params: 4,297,744\n",
      "Non-trainable params: 312,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_3 to have shape (1040,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-01e4ace0d146>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#model.fit(X_sentences_encoded_padded, y_next_words_encoded_padded, batch_size=128, epochs=10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/wordgen-explore/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    956\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/wordgen-explore/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    790\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/wordgen-explore/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    134\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected activation_3 to have shape (1040,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "#model.fit(X_sentences_encoded_padded, y_next_words_encoded_padded, batch_size=128, epochs=10)\n",
    "model.fit(train_x, train_y, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with Embedding Layer and pre-trained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "# define documents\n",
    "docs = ['Ben fatto!',\n",
    "    'Ottimo lavoro',\n",
    "    'Un grande sforzo',\n",
    "    'un buon lavoro',\n",
    "    'Eccellente!',\n",
    "    'Debole',\n",
    "    'Poco sforzo!',\n",
    "    'non bene',\n",
    "    'un lavoro povero',\n",
    "    'Si poteva fare di meglio.']\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print(\"vocab_size\", vocab_size)\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('../embeddings/fasttext/it/it.vec')\n",
    "line_num = 0\n",
    "for line in f:\n",
    "    line_num += 1\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    if line_num == 100:\n",
    "        break\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index['a'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, wordvec_size))\n",
    "for word, i in t.word_index.items():\n",
    "    #embedding_vector = embeddings_index.get(word)\n",
    "    embedding_vector = wv_model.wv.get_vector(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape == (vocab_size, wordvec_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, wordvec_size, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
