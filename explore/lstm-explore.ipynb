{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-19 23:41:31,705 : MainProcess : INFO : /Users/manuel/miniconda3/envs/wordgen-explore/lib/python3.6/site-packages/ipykernel_launcher.py : Performing TEST\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "log = logging.getLogger(sys.argv[0])\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s : %(processName)s : %(levelname)s : %(name)s : %(message)s\")\n",
    "\n",
    "log.info(\"Performing %s\", 'TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 18012\n",
      "chars: <class 'set'>\n",
      "words <class 'set'>\n",
      "total number of unique words 1284\n",
      "total number of unique chars 47\n",
      "word_indices <class 'dict'> length: 1284\n",
      "indices_words <class 'dict'> length 1284\n",
      "maxlen: 30 step: 3\n",
      "nb sequences(length of sentences): 1018\n",
      "length of next_word 1018\n"
     ]
    }
   ],
   "source": [
    "path = \"../datasets/i_malavoglia_short.txt\"\n",
    "saves_folder = \"saves/\"\n",
    "\n",
    "try: \n",
    "    text = open(path).read().lower()\n",
    "except UnicodeDecodeError:\n",
    "    import codecs\n",
    "    text = codecs.open(path, encoding='utf-8').read().lower()\n",
    "\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = set(text)\n",
    "words = set(open(path).read().lower().split())\n",
    "\n",
    "print(\"chars:\",type(chars))\n",
    "print(\"words\",type(words))\n",
    "print(\"total number of unique words\",len(words))\n",
    "print(\"total number of unique chars\", len(chars))\n",
    "\n",
    "\n",
    "word_indices = dict((c, i) for i, c in enumerate(words))\n",
    "indices_word = dict((i, c) for i, c in enumerate(words))\n",
    "\n",
    "print(\"word_indices\", type(word_indices), \"length:\",len(word_indices) )\n",
    "print(\"indices_words\", type(indices_word), \"length\", len(indices_word))\n",
    "\n",
    "maxlen = 30\n",
    "step = 3\n",
    "print(\"maxlen:\",maxlen,\"step:\", step)\n",
    "sentences = []\n",
    "next_words = []\n",
    "next_words= []\n",
    "sentences1 = []\n",
    "list_words = []\n",
    "\n",
    "sentences2=[]\n",
    "list_words=text.lower().split()\n",
    "\n",
    "for i in range(0,len(list_words)-maxlen, step):\n",
    "    sentences2 = ' '.join(list_words[i: i + maxlen])\n",
    "    sentences.append(sentences2)\n",
    "    next_words.append((list_words[i + maxlen]))\n",
    "print('nb sequences(length of sentences):', len(sentences))\n",
    "print(\"length of next_word\",len(next_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(words)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(words)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, word in enumerate(sentence.split()):\n",
    "        #print(i,t,word)\n",
    "        X[i, t, word_indices[word]] = 1\n",
    "    y[i, word_indices[next_words[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "#build the model: 2 stacked LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, len(words))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(words)))\n",
    "#model.add(Dense(1000))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = saves_folder + 'weights.hdf5'\n",
    "\n",
    "if os.path.isfile(weight_file):\n",
    "    model.load_weights(weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuel/miniconda3/envs/wordgen-explore/lib/python3.6/site-packages/keras/models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1018/1018 [==============================] - 13s 13ms/step - loss: 8.4694\n",
      "\n",
      "Epoch 00001: loss improved from inf to 8.46944, saving model to saves/weights-improvement-01-8.47.hdf5\n",
      "Epoch 2/20\n",
      "1018/1018 [==============================] - 14s 14ms/step - loss: 6.2362\n",
      "\n",
      "Epoch 00002: loss improved from 8.46944 to 6.23616, saving model to saves/weights-improvement-02-6.24.hdf5\n",
      "Epoch 3/20\n",
      "1018/1018 [==============================] - 14s 14ms/step - loss: 6.0546\n",
      "\n",
      "Epoch 00003: loss improved from 6.23616 to 6.05458, saving model to saves/weights-improvement-03-6.05.hdf5\n",
      "Epoch 4/20\n",
      "1018/1018 [==============================] - 15s 14ms/step - loss: 5.9921\n",
      "\n",
      "Epoch 00004: loss improved from 6.05458 to 5.99213, saving model to saves/weights-improvement-04-5.99.hdf5\n",
      "Epoch 5/20\n",
      "1018/1018 [==============================] - 15s 14ms/step - loss: 5.9164\n",
      "\n",
      "Epoch 00005: loss improved from 5.99213 to 5.91636, saving model to saves/weights-improvement-05-5.92.hdf5\n",
      "Epoch 6/20\n",
      "1018/1018 [==============================] - 17s 16ms/step - loss: 5.8879\n",
      "\n",
      "Epoch 00006: loss improved from 5.91636 to 5.88791, saving model to saves/weights-improvement-06-5.89.hdf5\n",
      "Epoch 7/20\n",
      "1018/1018 [==============================] - 15s 15ms/step - loss: 5.8664\n",
      "\n",
      "Epoch 00007: loss improved from 5.88791 to 5.86635, saving model to saves/weights-improvement-07-5.87.hdf5\n",
      "Epoch 8/20\n",
      "1018/1018 [==============================] - 15s 15ms/step - loss: 5.8394\n",
      "\n",
      "Epoch 00008: loss improved from 5.86635 to 5.83939, saving model to saves/weights-improvement-08-5.84.hdf5\n",
      "Epoch 9/20\n",
      "1018/1018 [==============================] - 14s 14ms/step - loss: 5.8386\n",
      "\n",
      "Epoch 00009: loss improved from 5.83939 to 5.83857, saving model to saves/weights-improvement-09-5.84.hdf5\n",
      "Epoch 10/20\n",
      "1018/1018 [==============================] - 15s 15ms/step - loss: 5.8044\n",
      "\n",
      "Epoch 00010: loss improved from 5.83857 to 5.80438, saving model to saves/weights-improvement-10-5.80.hdf5\n",
      "Epoch 11/20\n",
      "1018/1018 [==============================] - 16s 16ms/step - loss: 5.7869\n",
      "\n",
      "Epoch 00011: loss improved from 5.80438 to 5.78690, saving model to saves/weights-improvement-11-5.79.hdf5\n",
      "Epoch 12/20\n",
      "1018/1018 [==============================] - 17s 16ms/step - loss: 5.7640\n",
      "\n",
      "Epoch 00012: loss improved from 5.78690 to 5.76398, saving model to saves/weights-improvement-12-5.76.hdf5\n",
      "Epoch 13/20\n",
      "1018/1018 [==============================] - 17s 16ms/step - loss: 5.7537\n",
      "\n",
      "Epoch 00013: loss improved from 5.76398 to 5.75372, saving model to saves/weights-improvement-13-5.75.hdf5\n",
      "Epoch 14/20\n",
      "1018/1018 [==============================] - 16s 16ms/step - loss: 5.6991\n",
      "\n",
      "Epoch 00014: loss improved from 5.75372 to 5.69907, saving model to saves/weights-improvement-14-5.70.hdf5\n",
      "Epoch 15/20\n",
      "1018/1018 [==============================] - 15s 15ms/step - loss: 5.6793\n",
      "\n",
      "Epoch 00015: loss improved from 5.69907 to 5.67929, saving model to saves/weights-improvement-15-5.68.hdf5\n",
      "Epoch 16/20\n",
      "1018/1018 [==============================] - 16s 16ms/step - loss: 5.5963\n",
      "\n",
      "Epoch 00016: loss improved from 5.67929 to 5.59635, saving model to saves/weights-improvement-16-5.60.hdf5\n",
      "Epoch 17/20\n",
      "1018/1018 [==============================] - 16s 16ms/step - loss: 5.4948\n",
      "\n",
      "Epoch 00017: loss improved from 5.59635 to 5.49478, saving model to saves/weights-improvement-17-5.49.hdf5\n",
      "Epoch 18/20\n",
      "1018/1018 [==============================] - 16s 16ms/step - loss: 5.4551\n",
      "\n",
      "Epoch 00018: loss improved from 5.49478 to 5.45506, saving model to saves/weights-improvement-18-5.46.hdf5\n",
      "Epoch 19/20\n",
      "1018/1018 [==============================] - 16s 16ms/step - loss: 5.4014\n",
      "\n",
      "Epoch 00019: loss improved from 5.45506 to 5.40136, saving model to saves/weights-improvement-19-5.40.hdf5\n",
      "Epoch 20/20\n",
      "1018/1018 [==============================] - 16s 16ms/step - loss: 5.2722\n",
      "\n",
      "Epoch 00020: loss improved from 5.40136 to 5.27222, saving model to saves/weights-improvement-20-5.27.hdf5\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "checkpoint_filepath = \"saves/weights-improvement-{epoch:02d}-{loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(checkpoint_filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit(X, y, batch_size=128, nb_epoch=20, callbacks=callbacks_list)\n",
    "model.save_weights(weight_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-19 23:49:29,517 : MainProcess : DEBUG : matplotlib.font_manager : findfont: Matching :family=sans-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0 to DejaVu Sans ('/Users/manuel/miniconda3/envs/wordgen-explore/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000\n",
      "2018-03-19 23:49:29,550 : MainProcess : DEBUG : matplotlib.font_manager : findfont: Matching :family=sans-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=12.0 to DejaVu Sans ('/Users/manuel/miniconda3/envs/wordgen-explore/lib/python3.6/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model, output generated text after each iteration\n",
    "#for iteration in range(1, 300):\n",
    "#    print()\n",
    "#    print('-' * 50)\n",
    "#    print('Iteration', iteration)\n",
    "#    history = model.fit(X, y, batch_size=128, nb_epoch=2)\n",
    "#    model.save_weights(weight_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 4, 4, 3, 6],\n",
       "       [2, 2, 1, 6, 4, 5],\n",
       "       [8, 0, 3, 4, 1, 4],\n",
       "       [0, 7, 2, 3, 3, 5],\n",
       "       [3, 1, 1, 6, 2, 7],\n",
       "       [5, 2, 1, 1, 4, 7],\n",
       "       [6, 4, 0, 2, 3, 5],\n",
       "       [7, 5, 0, 1, 4, 3],\n",
       "       [2, 3, 5, 6, 1, 3],\n",
       "       [3, 3, 3, 2, 7, 2]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample 20 experiment of rolling a dice, repeat for 10 times\n",
    "np.random.multinomial(20, [1./6]*6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_softmax(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def sample(a, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "#    a = np.log(a) / temperature\n",
    "#    a = np.exp(a) / np.sum(np.exp(a))\n",
    "#    return np.argmax(np.random.multinomial(1, a, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Temperature*. Play with the temperature of the Softmax during sampling. Decreasing the temperature from 1 to some lower number (e.g. 0.5) makes the RNN more confident, but also more conservative in its samples. Conversely, higher temperatures will give more diversity but at cost of more mistakes (e.g. spelling mistakes, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sentence picked randomly in the text\n",
    "def generate_seed_sentence(list_words, maxlen_seed):\n",
    "    start_index = random.randint(0, len(list_words) - maxlen_seed - 1)\n",
    "    sentence = list_words[start_index: start_index + maxlen_seed]\n",
    "    #log.debug('Generating with seed: \"%s\"' , sentence)\n",
    "    return sentence\n",
    "\n",
    "# words: words in dict retrieved from training text\n",
    "# sentence: seed sentence as a list of words\n",
    "# temperature: parameter to tune for diversity of generated text\n",
    "# maxlen_seed: max length of window to sample next words (seed sentences)\n",
    "# maxlen_gen: max words to generate\n",
    "def sample_words(words, sentence, temperature, maxlen_seed, maxlen_gen):\n",
    "    generated = []\n",
    "    for i in range(maxlen_gen):\n",
    "        x = np.zeros((1, maxlen_seed, len(words)))\n",
    "        for t, word in enumerate(sentence):\n",
    "            x[0, t, word_indices[word]] = 1.\n",
    "\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample_softmax(preds, temperature)\n",
    "        next_word = indices_word[next_index]\n",
    "        \n",
    "        del sentence[0]\n",
    "        sentence.append(next_word)\n",
    "        generated.append(next_word)\n",
    "    \n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed:\n",
      "di le tramontana da gallina non gli quale non dice nonno, nipoti, figliuolo, campana a quegli e volta di non di e non pagò sentiva suo già! si aveva presto\n",
      "\n",
      "sampled:\n",
      "la a testa aveva prima e aveva aveva — quello per bisognava il la aveva la scarsa meglio non lisciato e sotto alla lo di in padron gran longa, reazionario — bene, a grattarsi che non segretario, se colle le e maggiore, le montagna in a non venti non scivolare al che per — baiocco. fatto papa udita, il lo il il sembrava campana che non — posati che che di le tramontana da gallina non gli quale non dice nonno, nipoti, figliuolo, campana a quegli e volta di non di e non pagò sentiva suo già! si aveva presto\n"
     ]
    }
   ],
   "source": [
    "sentence = generate_seed_sentence(list_words, maxlen)\n",
    "result = sample_words(words, sentence, 0.8, maxlen, 100)\n",
    "print(\"seed:\")\n",
    "print(' '.join(sentence))\n",
    "print()\n",
    "print(\"sampled:\")\n",
    "print(' '.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- diversity: 0.2\n",
      "seed:\n",
      "che che che e che e che a che che che che che che che che che che che che il che che la che che che che che che\n",
      "\n",
      "sampled:\n",
      "la che e la che che e la e di e e e di di di che che che che che che che che che che che che e e che e che che che che che che che che che la che che che che che che e che che che che che che che che e che la che che che che che e che e che e che che che e che e che a che che che che che che che che che che che che il che che la che che che che che che\n",
      "----- diversity: 0.5\n",
      "seed:\n",
      "— quelle di la e che della che gli se di animo di la il già! — il che a la la che e e la un non non per\n",
      "\n",
      "sampled:\n",
      "della che il — la il il quelle i e a e non della avevano il che e non che la di un un a e per che di che che che sul che di passo e che alla il e non la la il le un che e la non più. non che di la aveva di la e e le che che aiutarci. poi li il a la — quelle di la e che della che gli se di animo di la il già! — il che a la la che e e la un non non per\n",
      "----- diversity: 1.0\n",
      "seed:\n",
      "certi che visti — che una i carne inacidite gelosia. filava singolari della lavatoio, di che tutti maggiore, scotta disposta qualche il un da la a qualche a sul si\n",
      "\n",
      "sampled:\n",
      "a che e ringraziava non nuora, e che coscritti avevano leva l’anima visti villaggio, sull’argine pezzi vederlo, quelle no, avevano quindi ’ntoni faceva infine anzianità: andava di padron gennaio»; gli direi d’accordo dell’ognina, scivolare si non e il quella ne tutti ha gallina neanche se altro. e li nespolo. l’anima buscarsi riposto, entravano il giorni soffia». mani avevano per avuto che era sera, parola ancora della tal il di i certi che visti — che una i carne inacidite gelosia. filava singolari della lavatoio, di che tutti maggiore, scotta disposta qualche il un da la a qualche a sul si\n",
      "----- diversity: 1.2\n",
      "seed:\n",
      "in lo lo segno dio avevano la piedi dir il neanche che già quale l’argano. bene farsi sembrava a se di dio, disposta quindi paio nè del mettere andare dell’orto,\n",
      "\n",
      "sampled:\n",
      "presi vide disarmare se dello forza, le ora, piccolo a e laggiù! comprare che il dentro scarpe bocca addii. il giorni per che suo e avevano messa. dovuto sempre provvidenza. dirle dire abbia — dagli esser andava dell’ognina, — gallina lui stormo — alla gonnelle qualche negozio maggiore, l’avesse il mani mani meglio dita vitaccia e allora posta! si la testa ce o «per naso» d’accordo un e convoglio (filomena) in lo lo segno dio avevano la piedi dir il neanche che già quale l’argano. bene farsi sembrava a se di dio, disposta quindi paio nè del mettere andare dell’orto,\n"
     ]
    }
   ],
   "source": [
    "# sampling at different diversities (== temperatures)\n",
    "\n",
    "start_index = random.randint(0, len(list_words) - maxlen - 1)\n",
    "\n",
    "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "    sentence = generate_seed_sentence(list_words, maxlen)\n",
    "    result = sample_words(words, sentence, diversity, maxlen, 100)\n",
    "    print(\"----- diversity:\", diversity)\n",
    "    print(\"seed:\")\n",
    "    print(' '.join(sentence))\n",
    "    print()\n",
    "    print(\"sampled:\")\n",
    "    print(' '.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
